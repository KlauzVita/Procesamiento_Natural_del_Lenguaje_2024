{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Preprocesamiento de Tweets para el Análisis de Sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "\n",
    "Realizaremos un análisis de sentimientos de tweets usando el paquete [Kit de herramientas de lenguaje natural (NLTK)](http://www.nltk.org/howto/twitter.html), una biblioteca Python de código abierto para el procesamiento del lenguaje natural. \n",
    "\n",
    "Para este ejercicio, usaremos un conjunto de datos de Twitter que vienen con NLTK. Este conjunto de datos se ha anotado manualmente y sirve como base para desarrollar algunos modelos rápidamente. Vamos a importarlos ahora, así como algunas otras bibliotecas que usaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librería para NLP\n",
    "# Ejemplo de conjunto de datos de Twitter de NLTK\n",
    "# biblioteca para visualización\n",
    "# generador de números pseudoaleatorios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acerca del conjunto de datos de Twitter\n",
    "\n",
    "El conjunto de datos de muestra de NLTK se divide en tweets positivos y negativos. Contiene 5000 tweets positivos y 5000 tweets negativos exactamente. La intención es tener un conjunto de datos equilibrado. Eso no refleja las distribuciones reales de clases positivas y negativas en la base de datos que tiene Twitter. Es solo porque los conjuntos de datos equilibrados simplifican el diseño de la mayoría de los métodos computacionales que se requieren para el análisis de sentimientos. \n",
    "\n",
    "Para descargar los datos hay que hacer lo siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos cargar los campos de texto de los tweets positivos y negativos usando el método `strings ()` del módulo como este:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccione el conjunto de tweets positivos y negativos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, imprimiremos un informe con la cantidad de tweets positivos y negativos. También es fundamental conocer la estructura de datos de los conjuntos de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Podemos ver que los datos se almacenan en una lista y, como era de esperar, los tweets individuales se almacenan como cadenas.\n",
    "\n",
    "se puede hacer un reporte gráfico tilizando la librería [pyplot](https://matplotlib.org/tutorials/introductory/pyplot.html) de Matplotlib . Veamos cómo crear un gráfico circular para mostrar la misma información que la anterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración de los textos\n",
    "\n",
    "Antes que nada, podemos imprimir un par de tweets del conjunto de datos para ver cómo se ven. Comprender los datos nos da el 80% del éxito o fracaso en los proyectos de ineligencia artificial. Podemos utilizar este tiempo para observar aspectos que nos gustaría considerar al preprocesar nuestros datos.\n",
    "\n",
    "A continuación, imprimiremos un tweet positivo aleatorio y un tweet negativo aleatorio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprimir positivo en verde\n",
    "\n",
    "\n",
    "# imprimir positivo en rojo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una observación que puede tener es la presencia de [emoticones] (https://en.wikipedia.org/wiki/Emoticon) y URL en muchos de los tweets. Esta información será útil en los próximos pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesar previamente el texto para el análisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El preprocesamiento de datos es uno de los pasos críticos en cualquier proyecto de aprendizaje automático. Incluye limpiar y formatear los datos antes de introducirlos en un algoritmo de aprendizaje automático. Para NLP, los pasos de preprocesamiento se componen de las siguientes tareas:\n",
    "\n",
    "* Tokenizando la cadena\n",
    "* Minúsculas\n",
    "* Eliminación de stopwords\n",
    "* Stemming,lematización\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descargamos los stopwords de NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblioteca para operaciones de expresiones regulares\n",
    "# Operaciones con strings\n",
    "\n",
    "# módulo para palabras vacías que vienen con NLTK\n",
    "# módulo para stemming\n",
    "# módulo para tokenizar cadenas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar hipervínculos, marcas y estilos de Twitter\n",
    "\n",
    "Dado que tenemos un conjunto de datos de Twitter, nos gustaría eliminar algunas subcadenas de uso común en la plataforma, como el hashtag, las marcas de retweet y los hipervínculos. Usaremos la biblioteca [re](https://docs.python.org/3/library/re.html) para realizar operaciones de expresión regular en nuestro tweet. Definiremos nuestro patrón de búsqueda y usaremos el método `sub ()` para eliminar coincidencias sustituyéndolo con un carácter vacío (es decir, `` '' ``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizar la cadena\n",
    "\n",
    "Tokenizar significa dividir las cadenas en palabras individuales sin espacios en blanco ni tabulaciones. En este mismo paso, también convertiremos cada palabra de la cadena a minúsculas. El módulo [tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual) de NLTK nos permite hacer esto fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover stopwords y signos de puntuación\n",
    "\n",
    "El siguiente paso es eliminar los stopwords y signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Es hora de limpiar nuestro tweet tokenizado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tenga en cuenta que las palabras **happy** y **sunny** en esta lista están escritas correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "es el proceso de convertir una palabra a su forma más general, o raíz. Esto ayuda a reducir el tamaño de nuestro vocabulario.\n",
    "\n",
    "Considerando las siguientes palabras: \n",
    " * **learn**\n",
    " * **learn**ing\n",
    " * **learn**ed\n",
    " * **learn**t\n",
    " \n",
    "Todas estas palabras se derivan de su raíz común **aprender**. Sin embargo, en algunos casos, el proceso de derivación produce palabras que no tienen la ortografía correcta de la palabra raíz. Por ejemplo, **happi** y **sunni**. Por ejemplo, podemos observar el conjunto de palabras que componen las diferentes formas de `happy`:\n",
    "\n",
    " * **happ**y\n",
    " * **happi**ness\n",
    " * **happi**er\n",
    "\n",
    "Podemos ver que el prefijo **happi** se usa más comúnmente. No podemos elegir **happ** porque es la raíz de palabras no relacionadas como **happen**.\n",
    "\n",
    " \n",
    "NLTK tiene diferentes módulos para derivar y usaremos el módulo [PorterStemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)  que implementa el algoritmo [Porter Stemming Algorithm](https://tartarus.org/martin/PorterStemmer/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos un conjunto de palabras que podemos incorporar a la siguiente etapa de nuestro proyecto de aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process_tweet()\n",
    "\n",
    "Como se muestra arriba, el preprocesamiento consta de varios pasos antes de llegar a la lista final de palabras. Sin embargo, podemos  utilizar la función `process_tweet (tweet)` disponible en _utils.py_. \n",
    "\n",
    "Para obtener el mismo resultado que en las celdas de código anteriores, solo necesitará llamar a la función `process_tweet ()`. Hagámoslo en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
