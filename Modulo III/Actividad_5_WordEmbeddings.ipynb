{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86aOWqDfcrJq"
      },
      "source": [
        "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
        "\n",
        "# Actividad 6: WordEmbeddings (Modelo CBOW)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IYSALMdcrJs"
      },
      "source": [
        "\n",
        "En esta acividad, practicarás cómo calcular wordembedding y utilizarlas para el análisis de sentimientos.\n",
        "\n",
        "- Para implementar el análisis de sentimientos, podemos ir más allá de contar el número de palabras positivas y negativas.\n",
        "- Podrás encontrar una manera de representar cada palabra numéricamente, mediante un vector.\n",
        "- El vector podría entonces representar estructuras sintácticas (es decir, partes del discurso) y semánticas (es decir, significado).\n",
        "\n",
        "En esta actividad, explorarás una forma clásica de generar wordembeddings de palabras.\n",
        "- Implementarás un modelo famoso llamado modelo de bolsa continua de palabras (CBOW).\n",
        "\n",
        "\n",
        "Saber cómo entrenar estos modelos te brindará una mejor comprensión de los vectores palabras, que son los componentes básicos de muchas aplicaciones en el procesamiento del lenguaje natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEWrv5b_crJs"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Continuous bag of words (CBOW)\n",
        "\n",
        "Observemos la siguiente sentencia:\n",
        ">**'I am happy because I am learning'**.\n",
        "\n",
        "- En el modelado de bolsa continua de palabras (CBOW), intentamos predecir la palabra central dadas algunas palabras de contexto (las palabras alrededor de la palabra central).\n",
        "\n",
        "- Por ejemplo, si tuviera que elegir un contexto de tamaño medio, digamos $C = 2$, entonces intentaría predecir la palabra **happy** dado el contexto que incluye 2 palabras antes y 2 palabras después de la palabra central. :\n",
        "\n",
        "> $C$ words before: [I, am]\n",
        "\n",
        "> $C$ words after: [because, I]\n",
        "\n",
        "- en otras palabras:\n",
        "\n",
        "$$context = [I,am, because, I]$$\n",
        "$$target = happy$$\n",
        "\n",
        "La estructura del modelo se ve como sigue:\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='Figures/word2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 1 </div>\n",
        "\n",
        "dond $\\bar x$ es el promedio de todos los vectores one-hot de las palabras de contexto.\n",
        "\n",
        "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='mean_vec2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 2 </div>\n",
        "\n",
        "Una vez que ya tenemos los vectores contextos, podemos usar $\\bar x$  como la entrada al modelo\n",
        "\n",
        "La arquitectura a implementar es la siguiente:\n",
        "\n",
        "\\begin{align}\n",
        " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
        " a &= ReLU(h)  \\tag{2} \\\\\n",
        " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
        " \\hat y &= softmax(z)   \\tag{4} \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uj3fLKdOcrJt"
      },
      "outputs": [],
      "source": [
        "# Importar librerias y funciones de ayuda (utils2)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from utils2 import sigmoid, get_batches, compute_pca, get_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bG0TaS7pcrJt"
      },
      "outputs": [],
      "source": [
        "\n",
        "nltk.data.path.append('.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSmmc6pbdILP",
        "outputId": "f8ebd383-d246-4620-f162-94b67aba73f4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "anP_wpa7crJt"
      },
      "outputs": [],
      "source": [
        "# cargar y tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "I2f4-xeZcrJu"
      },
      "outputs": [],
      "source": [
        "# obtener la distribucion de frecuencias en el vocabulario\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n5nLEt3crJu"
      },
      "source": [
        "#### Mapear las palabras a sus indices y de indices a palabras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "vcUteXLIcrJu"
      },
      "outputs": [],
      "source": [
        "# get_dict obtener los diccionarios ind2vec, vec2ind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aJhAZ8UcrJv"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Entrenando el Modelo\n",
        "\n",
        "###  Inicialización del modelo\n",
        "\n",
        "Ahora inicializarás dos matrices y dos vectores.\n",
        "- La primera matriz ($W_1$) es de dimensión $N \\times V$, donde $V$ es el número de palabras en tu vocabulario y $N$ es la dimensión de tu vector de palabras.\n",
        "- La segunda matriz ($W_2$) es de dimensión $V \\times N$.\n",
        "- El vector $b_1$ tiene dimensiones $N\\times 1$\n",
        "- El vector $b_2$ tiene dimensiones $V\\times 1$.\n",
        "- $b_1$ y $b_2$ son los vectores bias.\n",
        "\n",
        "La estructura general del modelo se verá como en la Figura 1, pero en esta etapa solo estamos inicializando los parámetros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "fqHr4tdecrJv"
      },
      "outputs": [],
      "source": [
        "def initialize_model(N,V, random_seed=1):\n",
        "    '''\n",
        "    Inputs:\n",
        "        N:  dimension of hidden vector\n",
        "        V:  dimension of vocabulary\n",
        "        random_seed: random seed for consistent results in the unit tests\n",
        "     Outputs:\n",
        "        W1, W2, b1, b2: initialized weights and biases\n",
        "    '''\n",
        "\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "    # W1 shape (N,V)\n",
        "    W1 = np.random.rand(N,V)\n",
        "    # W2  shape (V,N)\n",
        "    W2 = np.random.rand(V,N)\n",
        "    # b1  shape (N,1)\n",
        "    b1 = np.random.rand(N,1)\n",
        "    # b2  shape (V,1)\n",
        "    b2 = np.random.rand(V,1)\n",
        "\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "BbqcMOpycrJv"
      },
      "outputs": [],
      "source": [
        "# Testear la función.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCeGebILcrJv"
      },
      "source": [
        "<a name='2.1'></a>\n",
        "### 2.1 Softmax\n",
        "Antes de que podamos comenzar a entrenar el modelo, debemos implementar la función softmax como se define en la ecuación 5:  \n",
        "\n",
        "<br>\n",
        "$$ \\text{softmax}(z_i) = \\frac{e^{z_i} }{\\sum_{i=0}^{V-1} e^{z_i} }  \\tag{5} $$\n",
        "\n",
        "- La indexación de matrices en el código comienza en 0.\n",
        "- $V$ es el número de palabras en el vocabulario (que también es el número de filas de $z$).\n",
        "- $i$ va de 0 a |V| - 1.\n",
        "\n",
        "\n",
        "**Implemente la función softmax a continuación. **\n",
        "\n",
        "- Supongamos que la entrada $z$ a `softmax` es una matriz 2D\n",
        "- Cada ejemplo de entrenamiento está representado por una columna de forma (V, 1) en esta matriz 2D.\n",
        "- Puede haber más de una columna en la matriz 2D, porque puede incluir un lote de ejemplos para aumentar la eficiencia. Llamemos al tamaño del lote $m$ en minúsculas, por lo que la matriz $z$ tiene la forma (V, m)\n",
        "- Al tomar la suma de $i=1 \\cdots V-1$, toma la suma de cada columna (cada ejemplo) por separado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7a95UwqPcrJw"
      },
      "outputs": [],
      "source": [
        "def softmax(z):\n",
        "    '''\n",
        "    Inputs:\n",
        "        z: output scores from the hidden layer\n",
        "    Outputs:\n",
        "        yhat: prediction (estimate of y)\n",
        "    '''\n",
        "\n",
        "\n",
        "    return yhat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EW3eMX50crJw"
      },
      "outputs": [],
      "source": [
        "# Testear la función"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIaUdUclcrJw"
      },
      "source": [
        "<a name='2.2'></a>\n",
        "### Forward propagation\n",
        "\n",
        "Implemente la propagación directa $z$ según las ecuaciones (1) a (3). <br>\n",
        "\n",
        "\\begin{align}\n",
        " h &= W_1 \\  X + b_1  \\tag{1} \\\\\n",
        " a &= ReLU(h)  \\tag{2} \\\\\n",
        " z &= W_2 \\  a + b_2   \\tag{3} \\\\\n",
        "\\end{align}\n",
        "\n",
        "Para ello utilizarás como función de activación la ReLU dada por::\n",
        "\n",
        "$$f(h)=\\max (0,h) \\tag{6}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "VOCUetsxcrJw"
      },
      "outputs": [],
      "source": [
        "def forward_prop(x, W1, W2, b1, b2):\n",
        "    '''\n",
        "    Inputs:\n",
        "        x:  average one hot vector for the context\n",
        "        W1, W2, b1, b2:  matrices and biases to be learned\n",
        "     Outputs:\n",
        "        z:  output score vector\n",
        "    '''\n",
        "\n",
        "\n",
        "    return z, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "ubhvcvMTcrJw"
      },
      "outputs": [],
      "source": [
        "# Testear la función\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W31B670crJx"
      },
      "source": [
        "\n",
        "## Función de Costo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8zh9ldeBcrJx"
      },
      "outputs": [],
      "source": [
        "# cross-entropy cost functioN\n",
        "def compute_cost(y, yhat, batch_size):\n",
        "    # cost function\n",
        "    logprobs = np.multiply(np.log(yhat),y) + np.multiply(np.log(1 - yhat), 1 - y)\n",
        "    cost = - 1/batch_size * np.sum(logprobs)\n",
        "    cost = np.squeeze(cost)\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LcsjmLtccrJx"
      },
      "outputs": [],
      "source": [
        "# Testear la función\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkAzo2tGcrJx"
      },
      "source": [
        "\n",
        "## Entrenar al modelo - Backpropagation\n",
        "\n",
        "Ahora que has entendido cómo funciona el modelo CBOW, lo entrenarás. <br>\n",
        "Creaste una función para la propagación hacia adelante. Ahora implementará una función que calcula los gradientes para propagar los errores hacia atrás."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(z):\n",
        "    result = z.copy()\n",
        "    result[result<0]=0\n",
        "    return result"
      ],
      "metadata": {
        "id": "3jh_OmGTg34n"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1=np.array([[0.52727857,  0.52727857,  0.52727857,  0.52727857],\n",
        " [-0.1259346,  -0.1259346,  -0.1259346 , -0.1259346 ],\n",
        " [ 0.39739328,  0.39739328,  0.39739328,  0.39739328],\n",
        " [-0.33644763, -0.33644763, -0.33644763, -0.33644763]])"
      ],
      "metadata": {
        "id": "qzN6_QE1hTYB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "oByUH6MCcrJx"
      },
      "outputs": [],
      "source": [
        "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
        "    '''\n",
        "    Inputs:\n",
        "        x:  average one hot vector for the context\n",
        "        yhat: prediction (estimate of y)\n",
        "        y:  target vector\n",
        "        h:  hidden vector (see eq. 1)\n",
        "        W1, W2, b1, b2:  matrices and biases\n",
        "        batch_size: batch size\n",
        "     Outputs:\n",
        "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases\n",
        "    '''\n",
        "\n",
        "\n",
        "    return grad_W1, grad_W2, grad_b1, grad_b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "vBYdDSSzcrJy"
      },
      "outputs": [],
      "source": [
        "# Testear la función\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8nD0_vOcrJy"
      },
      "source": [
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Ahora que ha implementado una función para calcular los gradientes, implementará el descenso de gradientes por **lotes** en su conjunto de entrenamiento.\n",
        "\n",
        "**Hint:** Para eso, usarás `initialize_model` y las funciones `back_prop` que acabas de crear (y la función `compute_cost`). También puedes utilizar la función auxiliar `get_batches` proporcionada:\n",
        "\n",
        "```for x, y in get_batches(data, word2Ind, V, C, batch_size):```\n",
        "\n",
        "```...```\n",
        "Además: imprima el costo después de procesar cada lote (use el tamaño de lote = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "O94VF0v4crJy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03):\n",
        "\n",
        "    '''\n",
        "    This is the gradient_descent function\n",
        "\n",
        "      Inputs:\n",
        "        data:      text\n",
        "        word2Ind:  words to Indices\n",
        "        N:         dimension of hidden vector\n",
        "        V:         dimension of vocabulary\n",
        "        num_iters: number of iterations\n",
        "     Outputs:\n",
        "        W1, W2, b1, b2:  updated matrices and biases\n",
        "\n",
        "    '''\n",
        "\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "KIhRMf1QcrJy"
      },
      "outputs": [],
      "source": [
        "# testear la función\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clDrvXLncrJz"
      },
      "source": [
        "\n",
        "## Visualización de los vectores palabra\n",
        "En esta parte visualizarás los vectores de palabras entrenados usando la función que acabas de codificar arriba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "9v8KSKUOcrJz"
      },
      "outputs": [],
      "source": [
        "# visualizar las palabras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "snBc5kuNcrJz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Jw0f6cmycrJz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6t6AAgSh71g"
      },
      "execution_count": 69,
      "outputs": []
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC2-4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}